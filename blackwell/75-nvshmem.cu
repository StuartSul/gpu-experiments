/*
    About using NVSHMEM:
    
        Requirements for SHARP & no Infiniband:
            - Hopper or later architecture
            - NVLink/NVSwitch-connected GPUs
            - 64-bit Linux (x86_64 or aarch64)
            - CUDA 12.1 or above
            - NVIDIA driver 530.30.02 or above

        Download:
            - Go to https://developer.nvidia.com/nvshmem-downloads
            - Or read https://docs.nvidia.com/nvshmem/release-notes-install-guide/install-guide/nvshmem-install-proc.html

        Compilation:
            - Use nvcc as usual
            - All headers under $(NVSHMEM_HOME)/include
                - For me, /usr/lib/x86_64-linux-gnu/nvshmem/12/libnvshmem.a
            - All libraries under $(NVSHMEM_HOME)/lib
                - For me, /usr/include/nvshmem_12/nvshmem(x).h
            - Following methods for compilation possible:
                - Use static linking for all: -lnvshmem (uses libnvshmem.a) and -lnvidia-ml
                - Use dynamic linking for the host: -lnvshmem_device -lnvshmem_host (libnvshmem_device.a + libnvshmem_host.so)
            - Also ensure to use -rdc=true
                - Without this (default behavior), the compiler will assume everything is within a single translation unit (TU; ie. a single .cu file)
                - With this, the compiler will generate relocatable device code (RDC) for each TU, and then call nvlink in addition to the
                host linker to link the RDCs together.

        Usage:
            - Include "nvshmem.h" and "nvshmemx.h"

        Launching:
            - Typical multi-gpu launch flow; a process manager launches multiple processes, each running the same executable
            - Launch with Hydra, Slurm, or OpenMPI (mpirun), or as part of OpenSHMEM application
            - You might need to install libhwloc15, which NVSHMEM uses to determine the system topology

        Some basics (https://docs.nvidia.com/nvshmem/api/index.html):
            - Processing element (PE): a single process (usually number of PEs = number of GPUs)
                - Each PE is given an integer ID, [0, num_pes - 1]
                - All PEs must simultaneously (=collectively) call nvshmem_init() & nvshmem_finalize()
            - Team: logical group of PEs
            - PEs communicate on "symmetric memory" that is allocated from "symmetric heap" which is located in GPU memory
                - This is allocated on the host-side
                    - Must be collectively allocated by all PEs using the same size argument; thus resulting memory is symmetric
                    - The returned address is also a valid local address for the current PE (i.e., can be used in plain CUDA code)
                    - In NVSHMEM ops, access is done via <symmetric_address, destination_PE>
                        - symmetric_address generated by performer pointer arithmetic (ex. &X[10] or &ptr->x)
                - Memory allocated by any other means is private to the allocating PE
                - The aggregation of all symmetric memory is referred to as PGAS (Partitioned Global Address Space)
            - Functions (supported on both CPU and GPU)
                - "get" and "put" operations
                    - Bulk/scalar/interleaved transfers are supported
                - Atomic Memory Operations (AMOs)
                - nvshmem_ptr gives a plain pointer, allowing the user to issue direct loads and stores
                - nvshmemx_mc_ptr gives a multicast pointer for a team, allowing the user to issue multimem load reduce and store broadcasts over a team
                - Synchronization (signal ops) also supported on both CPU and GPU
                    - All GPU sync (ex. nvshmem_barrier?)
                    - Thread-wise sync, through a point to point sync API (nvshmem_wait_until)
                - Collective functions (broadcast, reductions)
                - Wait and test
            - Fences
                - nvshmem_quiet: for both blocking and non-blocking ops
                    - All non-blocking operations issued by the calling PE will complete
                    - Accesses that occurred before nvshmem_quiet will be observed by all other PEs as before accesses after nvshmem_quiet
                    - Ordering is guaranteed for both API calls and direct loads/stores
                - nvshmem_fence: for blocking ops
                    - ** NVSHMEM does NOT guarantee ordering by default even on blocking ops within a single thread **
            - NVSHMEM is a stateful library (state stored in the runtime), users must abide by the following rules:
                - PE selects a GPU (ex. cudaSetDevice) before the first allocation
                - Allocation/synchronization is performed before any NVSHMEM call on the device
                - PE uses one and only one GPU
                - A GPU cannot be used by more than one PE
            - It is recommended to abide by the following:
                - Use coalesced access patterns
            - Undefined behavior when two operations access the same location without AMO or sync or wait/test
                - Writes are unordered, unless forced by nvshmem_fence or nvshmem_quiet
                - Reads are ordered by wait/test
                - Use nvshmem_quiet to make an update by PE ordered/visible to other PEs
                - All writes are eventually complete & remains visible once visible in one API call (stable)
*/

#include <mpi.h> // Crashes if not included first!
#include <cuda.h>
#include <nvshmem.h>
#include <nvshmemx.h>

// CUDA runtime API
#define CUDACHECK(cmd) do {                                   \
    cudaError_t err = cmd;                                    \
    if (err != cudaSuccess) {                                 \
        fprintf(stderr, "Failed: CUDA error %s:%d '%s'\n",    \
            __FILE__, __LINE__, cudaGetErrorString(err));     \
        exit(EXIT_FAILURE);                                   \
    }                                                         \
} while(0)

__global__ void simple_shift(int *dst) {
    int my_pe = nvshmem_my_pe();
    int num_pes = nvshmem_n_pes();
    int peer = (my_pe + 1) % num_pes;

    // Integer put operation
    nvshmem_int_p(dst, my_pe, peer);
}

int main(int argc, char **argv) {
    // Initialize MPI
    int rank;
    int world_size;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // Initialize NVSHMEM
    nvshmemx_init_attr_t attr = NVSHMEMX_INIT_ATTR_INITIALIZER;
    MPI_Comm mpi_comm = MPI_COMM_WORLD;
    attr.mpi_comm = &mpi_comm;
    nvshmemx_init_attr(NVSHMEMX_INIT_WITH_MPI_COMM, &attr);

    int current_pe_node = nvshmem_team_my_pe(NVSHMEMX_TEAM_NODE);
    CUDACHECK(cudaSetDevice(current_pe_node));

    cudaStream_t stream;
    cudaStreamCreate(&stream);

    int *dst = (int *)nvshmem_malloc(sizeof(int));

    simple_shift<<<1, 1, 0, stream>>>(dst);
    nvshmemx_barrier_all_on_stream(stream);

    int msg;
    cudaMemcpyAsync(&msg, dst, sizeof(int), cudaMemcpyDeviceToHost, stream);
    cudaStreamSynchronize(stream);

    printf("%d: received %d\n", nvshmem_my_pe(), msg);

    nvshmem_free(dst);
    cudaStreamDestroy(stream);

    nvshmem_finalize();
    MPI_Finalize();
    return 0;
}

// Below version assumes nvshmrun usage

// int main() {
//     nvshmem_init();
    
//     int my_pe_node = nvshmem_team_my_pe(NVSHMEMX_TEAM_NODE);
//     cudaSetDevice(my_pe_node);

//     cudaStream_t stream;
//     cudaStreamCreate(&stream);

//     int *dst = (int *)nvshmem_malloc(sizeof(int));

//     simple_shift<<<1, 1, 0, stream>>>(dst);
//     nvshmemx_barrier_all_on_stream(stream);

//     int msg;
//     cudaMemcpyAsync(&msg, dst, sizeof(int), cudaMemcpyDeviceToHost, stream);
//     cudaStreamSynchronize(stream);

//     printf("%d: received %d\n", nvshmem_my_pe(), msg);

//     nvshmem_free(dst);
//     cudaStreamDestroy(stream);

//     nvshmem_finalize();
//     return 0;
// }
