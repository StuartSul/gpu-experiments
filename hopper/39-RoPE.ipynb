{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a9a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7729f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "B = 8\n",
    "N = 32\n",
    "H = 4\n",
    "D_h = 16\n",
    "\n",
    "assert D_h % 2 == 0, \"D_h must be even\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bad8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sinosoidal position embeddings\n",
    "theta = np.power(10000, -2 * np.arange(D_h // 2) / D_h) # (D_h // 2,)\n",
    "\n",
    "sin_pos = np.sin(np.arange(N)[:, None] * theta[None, :]) # (N, D_h // 2)\n",
    "cos_pos = np.cos(np.arange(N)[:, None] * theta[None, :]) # (N, D_h // 2)\n",
    "\n",
    "sin_pos = np.repeat(sin_pos, repeats=2, axis=-1) # (N, D_h)\n",
    "cos_pos = np.repeat(cos_pos, repeats=2, axis=-1) # (N, D_h)\n",
    "\n",
    "sin_pos = sin_pos[np.newaxis, :, np.newaxis, :] # (1, N, 1, D_h)\n",
    "cos_pos = cos_pos[np.newaxis, :, np.newaxis, :] # (1, N, 1, D_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae1f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample inputs\n",
    "Q = np.random.randn(B, N, H, D_h)\n",
    "K = np.random.randn(B, N, H, D_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01089b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RoPE\n",
    "Q_sin = np.reshape(np.stack([\n",
    "    -Q[:, :, :, 1::2], Q[:, :, :, 0::2],\n",
    "], axis=-1), Q.shape)\n",
    "Q_rope = Q * cos_pos + Q_sin * sin_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1d472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference code from LlaMa, so we can check correctness\n",
    "# https://github.com/meta-llama/llama/blob/main/llama/model.py\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "\n",
    "    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "    and the end index 'end'. The 'theta' parameter scales the frequencies.\n",
    "    The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Dimension of the frequency tensor.\n",
    "        end (int): End index for precomputing frequencies.\n",
    "        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Precomputed frequency tensor with complex exponentials.\n",
    "    \"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Reshape frequency tensor for broadcasting it with another tensor.\n",
    "\n",
    "    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "    for the purpose of broadcasting the frequency tensor during element-wise operations.\n",
    "\n",
    "    Args:\n",
    "        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n",
    "        x (torch.Tensor): Target tensor for broadcasting compatibility.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reshaped frequency tensor.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the frequency tensor doesn't match the expected shape.\n",
    "        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n",
    "    \"\"\"\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n",
    "    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n",
    "    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\n",
    "    returned as real tensors.\n",
    "\n",
    "    Args:\n",
    "        xq (torch.Tensor): Query tensor to apply rotary embeddings.\n",
    "        xk (torch.Tensor): Key tensor to apply rotary embeddings.\n",
    "        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n",
    "    \"\"\"\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "810fbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reference RoPE\n",
    "Q_torch = torch.from_numpy(Q).float()\n",
    "K_torch = torch.from_numpy(K).float()\n",
    "freqs_cis = precompute_freqs_cis(D_h, N, 10000.)\n",
    "Q_ref = apply_rotary_emb(Q_torch, K_torch, freqs_cis)[0].numpy() # just look at Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ef1ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 32, 4, 16) (8, 32, 4, 16)\n",
      "Max diff: 1.6713005630553113e-06\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "assert Q_ref.shape == Q_rope.shape\n",
    "print(Q_ref.shape, Q_rope.shape)\n",
    "abs_diff = np.abs(Q_ref - Q_rope)\n",
    "max_diff = np.max(abs_diff)\n",
    "assert max_diff < 1e-5\n",
    "print(\"Max diff:\", max_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
